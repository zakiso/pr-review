#!/usr/bin/env python3
"""
LLM-based PR Validator
----------------------
Uses OpenAI's API to evaluate PR quality and provide improvement suggestions.
"""

import os
import sys
import json
import time
from openai import OpenAI

def evaluate_pr_with_llm(title, body):
    """Evaluate PR quality using OpenAI API"""
    # Get API key and model from env vars
    api_key = os.environ.get('OPENAI_API_KEY')
    model_name = os.environ.get('MODEL_NAME', 'gpt-4')

    if not api_key:
        print("âŒ Error: OPENAI_API_KEY environment variable is not set")
        with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
            f.write("llm_check_title=LLM è¯„ä¼°å¤±è´¥\n")
            f.write("llm_check_summary=ç¼ºå°‘ OpenAI API å¯†é’¥ã€‚\n")
            f.write("llm_check_text=è¯·é…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ã€‚\n")
            f.write("llm_check_conclusion=failure\n")
        sys.exit(1)
    
    # Initialize OpenAI client
    client = OpenAI(api_key=api_key)
    
    # Create the prompt for the LLM
    prompt = f"""
You are an expert code reviewer tasked with evaluating the quality of a GitHub Pull Request.
Analyze the following PR title and description to determine if it meets high-quality standards.

PR Title: {title}
PR Description:
{body}

Evaluate based on these criteria:
1. Clarity: Is the purpose of the PR clearly communicated?
2. Completeness: Does it explain what changes were made and why?
3. Technical Detail: Are implementation details sufficiently explained?
4. Testing: Is there information about how the changes were tested?

Respond with a JSON object containing:
{{
  "quality_score": [1-10 integer score],
  "is_acceptable": [boolean, true if score >= 6],
  "strengths": [array of strengths],
  "improvement_suggestions": [array of specific suggestions for improvement],
  "explanation": [brief explanation of your evaluation]
}}
"""

    # Maximum retries for API call
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            print(f"ğŸ¤– ä½¿ç”¨ {model_name} è¯„ä¼° PR...")
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            # Extract the response content
            content = response.choices[0].message.content
            
            try:
                result = json.loads(content)
                return result
            except json.JSONDecodeError:
                print(f"Error parsing JSON from API response: {content[:200]}...")
                retry_count += 1
                time.sleep(2)
                
        except Exception as e:
            print(f"Error calling OpenAI API: {e}")
            retry_count += 1
            time.sleep(2)
    
    print("âŒ Failed to get a valid response from the OpenAI API after multiple retries")
    with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
        f.write("llm_check_title=LLM è¯„ä¼°å¤±è´¥\n")
        f.write("llm_check_summary=æ— æ³•è·å–æœ‰æ•ˆçš„ LLM å“åº”ã€‚\n")
        f.write("llm_check_text=åœ¨å¤šæ¬¡é‡è¯•åä»æ— æ³•ä» OpenAI API è·å–æœ‰æ•ˆå“åº”ã€‚è¯·æ£€æŸ¥ API è¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§ã€‚\n")
        f.write("llm_check_conclusion=failure\n")
    sys.exit(1)

def format_feedback_text(result):
    """Format the LLM feedback as a report text"""
    emoji_map = {
        1: "ğŸš¨", 2: "ğŸš¨", 3: "ğŸš¨", 4: "âš ï¸", 5: "âš ï¸",
        6: "ğŸ‘", 7: "ğŸ‘", 8: "âœ…", 9: "ğŸŒŸ", 10: "ğŸŒŸ"
    }
    
    score = result["quality_score"]
    emoji = emoji_map.get(score, "ğŸ”")
    
    report_text = f"""
## {emoji} PR è´¨é‡è¯„ä¼°

**è¯„åˆ†: {score}/10** - {result["explanation"]}

### ä¼˜ç‚¹
{chr(10).join([f"- {s}" for s in result["strengths"]])}

### æ”¹è¿›å»ºè®®
{chr(10).join([f"- {s}" for s in result["improvement_suggestions"]])}

---
*æ­¤è¯„ä¼°ç”± AI ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚*
"""
    return report_text

def main():
    """Main function to validate PR using LLM"""
    if len(sys.argv) < 2:
        print("Usage: python llm_check.py <PR title> [PR body]")
        sys.exit(1)
    
    pr_title = sys.argv[1]
    pr_body = sys.argv[2] if len(sys.argv) > 2 else ""
    
    print(f"DEBUG: è¯„ä¼° PR æ ‡é¢˜: '{pr_title}'")
    print(f"DEBUG: PR æè¿°é•¿åº¦: {len(pr_body)} å­—ç¬¦")
    
    # Run LLM evaluation
    evaluation = evaluate_pr_with_llm(pr_title, pr_body)
    
    # Print results
    print(f"ğŸ¤– LLM è´¨é‡è¯„åˆ†: {evaluation['quality_score']}/10")
    print(f"ğŸ¤– æ˜¯å¦å¯æ¥å—: {'æ˜¯' if evaluation['is_acceptable'] else 'å¦'}")
    print("\nğŸ¤– ä¼˜ç‚¹:")
    for strength in evaluation['strengths']:
        print(f"  - {strength}")
    
    print("\nğŸ¤– æ”¹è¿›å»ºè®®:")
    for suggestion in evaluation['improvement_suggestions']:
        print(f"  - {suggestion}")
    
    print(f"\nğŸ¤– è¯„ä»·: {evaluation['explanation']}")
    
    # Format the feedback
    report_text = format_feedback_text(evaluation)
    
    # Set output variables for GitHub check
    with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
        if evaluation['is_acceptable']:
            f.write("llm_check_title=PR è´¨é‡è¯„ä¼°é€šè¿‡\n")
            f.write(f"llm_check_summary=PR è´¨é‡è¯„åˆ†: {evaluation['quality_score']}/10ï¼Œè¾¾åˆ°åˆæ ¼æ ‡å‡†ã€‚\n")
            f.write("llm_check_conclusion=success\n")
        else:
            f.write("llm_check_title=PR è´¨é‡è¯„ä¼°æœªé€šè¿‡\n")
            f.write(f"llm_check_summary=PR è´¨é‡è¯„åˆ†: {evaluation['quality_score']}/10ï¼Œæœªè¾¾åˆ°åˆæ ¼æ ‡å‡†ã€‚\n")
            f.write("llm_check_conclusion=failure\n")
        
        f.write("llm_check_text<<EOF\n")
        f.write(f"{report_text}\n")
        f.write("EOF\n")
    
    # Exit with appropriate status code
    if not evaluation['is_acceptable']:
        print("\nâŒ PR è´¨é‡ä¸ç¬¦åˆæœ€ä½æ ‡å‡†ã€‚è¯·å‚è€ƒä¸Šè¿°å»ºè®®ã€‚")
        sys.exit(1)
    else:
        print("\nâœ… PR è´¨é‡ç¬¦åˆæœ€ä½æ ‡å‡†ã€‚")
        sys.exit(0)

if __name__ == "__main__":
    main()